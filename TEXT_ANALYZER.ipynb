{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ES_CeydDIZOVXvg4zGw-nKvEEaDlHJuH",
      "authorship_tag": "ABX9TyOOA5oc9cM9XNodZJleV5tN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InukshiSenarathne/chatbot_colab_testing/blob/main/TEXT_ANALYZER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPIoq9bAQMb2"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "from spacy.pipeline import DependencyParser\n",
        "from spacy.lang.en import English\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Step 1: Preprocessing the Data\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation and symbols\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    \n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Step 2: Splitting the Data\n",
        "def split_data(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Step 3: Building the Models\n",
        "def train_models(X_train, y_train):\n",
        "    models = []\n",
        "    \n",
        "    # Random Forest Regression\n",
        "    rf_model = RandomForestRegressor()\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    models.append(('Random Forest', rf_model))\n",
        "    \n",
        "    # K-Nearest Neighbors Regression\n",
        "    knn_model = KNeighborsRegressor()\n",
        "    knn_model.fit(X_train, y_train)\n",
        "    models.append(('KNN', knn_model))\n",
        "    \n",
        "    # Gradient Boosting Regressor\n",
        "    gb_model = GradientBoostingRegressor()\n",
        "    gb_model.fit(X_train, y_train)\n",
        "    models.append(('Gradient Boosting', gb_model))\n",
        "    \n",
        "    return models\n",
        "\n",
        "def evaluate_models(models, X_test, y_test):\n",
        "    for name, model in models:\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred = np.around(y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='macro')\n",
        "        recall = recall_score(y_test, y_pred, average='macro')\n",
        "        f1 = f1_score(y_test, y_pred, average='macro')\n",
        "        print(f'{name} - Precision: {0}, Recall: {recall}, F1-score: {f1}')\n",
        "\n",
        "# Step 4: Plotting the Graphs\n",
        "def plot_performance(models, X_test, y_test):\n",
        "    names = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    \n",
        "    for name, model in models:\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred = np.around(y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')\n",
        "        recall = recall_score(y_test, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "    \n",
        "    x = np.arange(len(names))\n",
        "    width = 0.2\n",
        "    \n",
        "    fig, ax = plt.subplots()\n",
        "    ax.bar(x - width, precisions, width, label='Precision')\n",
        "    ax.bar(x, recalls, width, label='Recall')\n",
        "    ax.bar(x + width, f1_scores, width, label='F1-score')\n",
        "    \n",
        "    ax.set_ylabel('Scores')\n",
        "    ax.set_title('Model Performance')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(names)\n",
        "    ax.legend()\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "# Step 5: Prediction Model\n",
        "def train_prediction_model(X, y):\n",
        "    # Select the best-performing model (Random Forest) based on evaluation results\n",
        "    rf_model = RandomForestRegressor()\n",
        "    rf_model.fit(X, y)\n",
        "    return rf_model\n",
        "\n",
        "def predict_traits(model, text):\n",
        "    # Preprocess the text\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "    \n",
        "    # Vectorize the preprocessed text\n",
        "    vectorized_text = vectorizer.transform([preprocessed_text])\n",
        "    \n",
        "    # Predict the traits\n",
        "    traits = model.predict(vectorized_text)\n",
        "    \n",
        "    return traits\n",
        "\n",
        "# Excecution Start\n",
        "# Load JSON object\n",
        "with open('drive/MyDrive/research_project_y4_sliit/sample_data/interview_data.json') as f:\n",
        "    data = json.load(f)\n",
        "    text_data = data['text']\n",
        "\n",
        "# Preprocess the text data\n",
        "preprocessed_data = [preprocess_text(text) for text in text_data]\n",
        "\n",
        "# Vectorize the preprocessed data\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(preprocessed_data)\n",
        "\n",
        "# Generate random labels for demonstration purposes (replace with actual labels)\n",
        "y = np.random.randint(1, 6, len(text_data))\n",
        "print(y)\n",
        "# y = ['Kind', '']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = split_data(X, y)\n",
        "\n",
        "# Train the models\n",
        "models = train_models(X_train, y_train)\n",
        "\n",
        "# Evaluate the models\n",
        "evaluate_models(models, X_test, y_test)\n",
        "\n",
        "# Plot the performance of the models\n",
        "#plot_performance(models, X_test, y_test)\n",
        "\n",
        "# Train the prediction model\n",
        "prediction_model = train_prediction_model(X, y)\n",
        "\n",
        "# Predict traits for new candidates\n",
        "new_text = \"I am a highly motivated individual who enjoys working in a team.\"\n",
        "traits = predict_traits(prediction_model, new_text)\n",
        "print(f\"Predicted traits: {traits}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJfP4RwvQRLo",
        "outputId": "cc7f4345-7410-440e-ada4-6959c6f8f2a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 3 1 5 4 4 2 1 4 1 2 1 3 1 3 1 2 4 1 4 4 2 1 4 4 1 1 4 1 1 1 5 1 2]\n",
            "Random Forest - Precision: 0, Recall: 0.3333333333333333, F1-score: 0.16666666666666666\n",
            "KNN - Precision: 0, Recall: 0.0, F1-score: 0.0\n",
            "Gradient Boosting - Precision: 0, Recall: 0.3333333333333333, F1-score: 0.09523809523809523\n",
            "Predicted traits: [1.77]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "from spacy.pipeline import DependencyParser\n",
        "from spacy.lang.en import English\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Step 1: Preprocessing the Data\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation and symbols\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "    return text\n",
        "\n",
        "# Step 2: Splitting the Data\n",
        "def split_data(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Step 3: Building the Models\n",
        "def train_models(X_train, y_train):\n",
        "    models = []\n",
        "\n",
        "    # Random Forest Regression\n",
        "    rf_model = RandomForestRegressor()\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    models.append(('Random Forest', rf_model))\n",
        "\n",
        "    # K-Nearest Neighbors Regression\n",
        "    knn_model = KNeighborsRegressor()\n",
        "    knn_model.fit(X_train, y_train)\n",
        "    models.append(('KNN', knn_model))\n",
        "\n",
        "    # Gradient Boosting Regressor\n",
        "    gb_model = GradientBoostingRegressor()\n",
        "    gb_model.fit(X_train, y_train)\n",
        "    models.append(('Gradient Boosting', gb_model))\n",
        "\n",
        "    return models\n",
        "\n",
        "def evaluate_models(models, X_test, y_test):\n",
        "    for name, model in models:\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred = np.around(y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='macro')\n",
        "        recall = recall_score(y_test, y_pred, average='macro')\n",
        "        f1 = f1_score(y_test, y_pred, average='macro')\n",
        "        print(f'{name} - Precision: {precision}, Recall: {recall}, F1-score: {f1}')\n",
        "\n",
        "# Step 4: Plotting the Graphs\n",
        "def plot_performance(models, X_test, y_test):\n",
        "    names = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for name, model in models:\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred = np.around(y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')\n",
        "        recall = recall_score(y_test, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    x = np.arange(len(names))\n",
        "    width = 0.2\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.bar(x - width, precisions, width, label='Precision')\n",
        "    ax.bar(x, recalls, width, label='Recall')\n",
        "    ax.bar(x + width, f1_scores, width, label='F1-score')\n",
        "\n",
        "    ax.set_ylabel('Scores')\n",
        "    ax.set_title('Model Performance')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(names)\n",
        "    ax.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Step 5: Prediction Model\n",
        "def train_prediction_model(X, y):\n",
        "    # Select the best-performing model (Random Forest) based on evaluation results\n",
        "    rf_model = RandomForestRegressor()\n",
        "    rf_model.fit(X, y)\n",
        "    return rf_model\n",
        "\n",
        "def predict_traits(model, text):\n",
        "    # Preprocess the text\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "\n",
        "    # Vectorize the preprocessed text\n",
        "    vectorized_text = vectorizer.transform([preprocessed_text])\n",
        "\n",
        "    # Predict the traits\n",
        "    traits = model.predict(vectorized_text)\n",
        "\n",
        "    return traits\n",
        "\n",
        "# Load JSON object\n",
        "with open('drive/MyDrive/research_project_y4_sliit/sample_data/interview_data.json') as f:\n",
        "    data = json.load(f)\n",
        "    text_data = data['text']\n",
        "\n",
        "# Preprocess the text data\n",
        "preprocessed_data = [preprocess_text(text) for text in text_data]\n",
        "\n",
        "# Vectorize the preprocessed data\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(preprocessed_data)\n",
        "\n",
        "# Load the labeled dataset for Big Five personality traits\n",
        "with open('drive/MyDrive/research_project_y4_sliit/sample_data/labled_data.json') as f:\n",
        "    labeled_data = json.load(f)\n",
        "    labels = labeled_data['labels']\n",
        "\n",
        "# Convert labels to NumPy array\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = split_data(X, y)\n",
        "\n",
        "# Train the models\n",
        "models = train_models(X_train, y_train)\n",
        "\n",
        "# Evaluate the models\n",
        "evaluate_models(models, X_test, y_test)\n",
        "\n",
        "# Plot the performance of the models\n",
        "# plot_performance(models, X_test, y_test)\n",
        "\n",
        "# Train the prediction model\n",
        "prediction_model = train_prediction_model(X, y)\n",
        "\n",
        "# Predict traits for new candidates\n",
        "new_text = \"I am a highly motivated individual who enjoys working in a team.\"\n",
        "traits = predict_traits(prediction_model, new_text)\n",
        "print(f\"Predicted traits: {traits}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "z1rSvFB7XoMm",
        "outputId": "09b02b33-d9a6-4340-be7d-b4e024e56290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-12552f93c35e>\u001b[0m in \u001b[0;36m<cell line: 143>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;31m# Split the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;31m# Train the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-12552f93c35e>\u001b[0m in \u001b[0;36msplit_data\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Step 2: Splitting the Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2559\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [34, 2]"
          ]
        }
      ]
    }
  ]
}